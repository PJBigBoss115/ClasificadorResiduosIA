# ClasificadorResiduosIA - Inteligencia Artificial para la Gesti칩n de Residuos Urbanos
ClasificadorResiduosIA es un proyecto de inteligencia artificial dise침ado para automatizar la clasificaci칩n de residuos reciclables. Utilizando el poder de YOLOv8 y deep learning, este sistema identifica y clasifica desechos comunes como pl치stico, aluminio y cart칩n, facilitando la correcta disposici칩n y promoviendo pr치cticas sostenibles.

Implementado en una Nvidia Jetson Nano, el proyecto est치 optimizado para su uso en entornos de recursos limitados, ofreciendo una soluci칩n eficiente y escalable para la gesti칩n de residuos en entornos urbanos e industriales.

Caracter칤sticas principales:
- 游 Reconocimiento de im치genes en tiempo real utilizando YOLOv8.
- 鮫勇 Clasificaci칩n autom치tica de residuos reciclables: pl치stico, aluminio y cart칩n.
- 游눹 Implementaci칩n ligera y optimizada para dispositivos embebidos como Nvidia Jetson Nano.
- 游늵 Registro detallado de las clasificaciones para su an치lisis y mejora continua.
- 游 F치cil de configurar y desplegar en otros entornos.

## Requisitos T칠cnicos

|                    | CPU                            | RAM        | DISCO        | GPU                    | Sistema Operativo                |
|--------------------|--------------------------------|------------|--------------|------------------------|----------------------------------|
| Minimos            | Ryzen / Core 3 (> 2020)        | 4 GB       | 6 GB Espacio | Integrado              | Ubuntu 22.04 LTS / Windows 10    |
| Recomendados       | Ryzen / Core 5 (> 2022)        | 8 GB       | 8 GB Espacio | NVIDIA GeForce GT 1030 | Ubuntu 24.04 LTS / Windows 11    |
| Nvidia Jetson Nano | ARMv8 Processor rev 1 (v8l) x2 | 4 GB       | 8 GB Espacio | NVIDIA Tegra X1 (nvgpu)| Ubuntu 18.04 LTS                 |

## Instalaci칩n del Modelo

1. Descarga del Modelo
El primer paso es obtener los archivos del modelo.

2. Instalaci칩n de Dependencias
Es importante instalar todas las dependencias necesarias, est치n detalladas en un archivo requirements.txt.

      a. Crear y Activar el Entorno Virtual:
      Crear un entorno virtual asegura que todas las dependencias se manejen de forma aislada para evitar conflictos con otras aplicaciones.
   
      ```bash
      # Crear el entorno virtual
      python3 -m venv env
        
      # Activar el entorno virtual
      # En Linux/MacOS:
      source env/bin/activate
        
      # En Windows:
      .\env\Scripts\activate
      ```

      b. Instalar las Dependencias:
      Una vez activado el entorno virtual, instala las dependencias desde el archivo requirements.txt.

      ```bash
      pip install -r requirements.txt
      ```

3. Ejecuci칩n del Modelo
Una vez que tienes el modelo descargado, descomprimido y todas las dependencias instaladas, puedes proceder a la ejecuci칩n del modelo.

      a. Ejecuci칩n desde la Terminal:
      Linux:
      ```bash
      python script.py --model /ruta/del/modelo/modelo_entrenado.pt --source 0
      ```
      Windows:
      ```bash
      python script.py --model C:\ruta\del\modelo\modelo_entrenado.pt --source 0
      ```

      b. Ejecuci칩n Desde C칩digo (usando Python):
      Si prefieres ejecutar el modelo desde c칩digo, aseg칰rate de que el entorno virtual est칠 activado y ejecuta el script directamente desde un archivo .py.
       Si ejecutas desde la terminal el modelo solo empezara la deteccion y guardara peque침as capturas de sus detecciones, si ejecutas el codigo este
       abrira una ventana donde estara mostrando en tiempo real las detecciones realizadas por el modelo.
   
      ```python
      import cv2
      from ultralytics import YOLO
      
      # Cargar el modelo la ruta debe coresponder al lugar donde tienes almacenado
      # el proyecto por lo cual aqui esta incompleta
      model = YOLO('/runs/detect/train3/weights/best.pt')
      
      # Inicializar la c치mara, por defecto la camara web (integrada) esta en el '0'
      # pero otras externas podrian estar en el 3 o 4 (USB port)
      cap = cv2.VideoCapture(0)
      
      while True:
          ret, frame = cap.read()
      
          if not ret:
              print("No se puede recibir frames. Saliendo...")
              break
      
          results = model(frame)
      
          for result in results:
              if hasattr(result, 'boxes'):
                  detections = result.boxes
      
                  for detection in detections:
                      confidence = detection.conf.item()
                      if confidence > 0.2:
      
                          startX, startY, endX, endY = map(int, detection.xyxy[0])
                          label = f"Class {int(detection.cls.item())}: {confidence:.2f}"
      
                          cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)
                          y = startY - 15 if startY - 15 > 15 else startY + 15
                          cv2.putText(frame, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
      
          # Mostrar las detecciones
          cv2.imshow("Detecciones en tiempo real", frame)

          # Cerrar al presionar q
          if cv2.waitKey(1) & 0xFF == ord('q'):
              break
      
      cap.release()
      cv2.destroyAllWindows()
      ```

## Funcionamiento del Modelo

![Collage del Modelo](runs/detect/train3/val_batch1_pred.jpg)

La implementaci칩n del modelo puede variar significativamente seg칰n el objetivo que se desea alcanzar, ya que los modelos de aprendizaje profundo son flexibles y adaptables a m칰ltiples escenarios. Dependiendo del caso de uso, como la detecci칩n de objetos, clasificaci칩n de im치genes, reconocimiento facial, entre otros, se pueden ajustar los par치metros y la configuraci칩n del modelo para optimizar su desempe침o.

Adem치s, es posible aprovechar el entrenamiento previamente realizado, un enfoque conocido como transfer learning. Esto permite utilizar un modelo ya entrenado en una gran base de datos y ajustarlo para un nuevo conjunto de datos m치s espec칤fico. Este proceso reduce significativamente el tiempo de entrenamiento, ya que el modelo reutiliza el conocimiento adquirido en tareas anteriores, ajustando 칰nicamente las capas finales para aprender las nuevas caracter칤sticas. Esto es especialmente 칰til cuando se tiene acceso limitado a datos o recursos computacionales.

## Resoluci칩n de Problemas

### Problema 1: La ventana de detecci칩n no se abre
  Soluci칩n:
  1. Verifica que la c치mara est칠 correctamente conectada y funcionando.
  2. Aseg칰rate de que no haya otro programa utilizando la c치mara.
  3. Revisa los permisos de la c치mara en tu sistema operativo.

### Problema 2: Errores de Importaci칩n
  Soluci칩n:
  1. Aseg칰rate de que el entorno virtual est칠 activado.
  2. Verifica que todas las dependencias est칠n instaladas correctamente con pip install -r requirements.txt.

### Problema 3: El modelo no detecta objetos correctamente
  Soluci칩n:
  1. Aseg칰rate de el archivo .pt est칠 correctamente cargado.
  2. Ajusta el umbral de confianza seg칰n sea necesario.

### Logs y Diagn칩sticos
  Revisa la consola para mensajes de error o advertencias que puedan indicar la causa del problema. Utiliza herramientas de depuraci칩n si es necesario.

## Agradecimientos
- Gracias a la comunidad de Ultralytics por el desarrollo de YOLOv8.
- Inspiraci칩n y soporte de mis colegas y mentores en el desarrollo de este proyecto.
